{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "diabetes = pd.read_csv('D:/Vit/Semester_5/MachineLearning/Lab/pima-indians-diabetes-database/diabetes.csv', na_values=['NA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            1       85             66             29        0  26.6   \n",
       "1            1       89             66             23       94  28.1   \n",
       "2            5      116             74              0        0  25.6   \n",
       "3           10      115              0              0        0  35.3   \n",
       "4            4      110             92              0        0  37.6   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.351   31        0  \n",
       "1                     0.167   21        0  \n",
       "2                     0.201   30        0  \n",
       "3                     0.134   29        0  \n",
       "4                     0.191   30        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = diabetes.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>139</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>83</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>126</td>\n",
       "      <td>88</td>\n",
       "      <td>41</td>\n",
       "      <td>235</td>\n",
       "      <td>39.3</td>\n",
       "      <td>0.704</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>99</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.4</td>\n",
       "      <td>0.388</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "      <td>140</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0.487</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>145</td>\n",
       "      <td>82</td>\n",
       "      <td>19</td>\n",
       "      <td>110</td>\n",
       "      <td>22.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>117</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.1</td>\n",
       "      <td>0.337</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>109</td>\n",
       "      <td>75</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.546</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>58</td>\n",
       "      <td>11</td>\n",
       "      <td>54</td>\n",
       "      <td>24.8</td>\n",
       "      <td>0.267</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>0.188</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>122</td>\n",
       "      <td>78</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.512</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>103</td>\n",
       "      <td>60</td>\n",
       "      <td>33</td>\n",
       "      <td>192</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.966</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11</td>\n",
       "      <td>138</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.2</td>\n",
       "      <td>0.420</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>64</td>\n",
       "      <td>25</td>\n",
       "      <td>70</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.271</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7</td>\n",
       "      <td>133</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.2</td>\n",
       "      <td>0.696</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7</td>\n",
       "      <td>106</td>\n",
       "      <td>92</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>22.7</td>\n",
       "      <td>0.235</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7</td>\n",
       "      <td>159</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.4</td>\n",
       "      <td>0.294</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.564</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.586</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.305</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>80</td>\n",
       "      <td>11</td>\n",
       "      <td>82</td>\n",
       "      <td>19.4</td>\n",
       "      <td>0.491</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>50</td>\n",
       "      <td>15</td>\n",
       "      <td>36</td>\n",
       "      <td>24.2</td>\n",
       "      <td>0.526</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5</td>\n",
       "      <td>88</td>\n",
       "      <td>66</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.342</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>66</td>\n",
       "      <td>42</td>\n",
       "      <td>342</td>\n",
       "      <td>34.7</td>\n",
       "      <td>0.718</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>1</td>\n",
       "      <td>149</td>\n",
       "      <td>68</td>\n",
       "      <td>29</td>\n",
       "      <td>127</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.349</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>78</td>\n",
       "      <td>23</td>\n",
       "      <td>79</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0.323</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>8</td>\n",
       "      <td>120</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0.259</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>88</td>\n",
       "      <td>37</td>\n",
       "      <td>120</td>\n",
       "      <td>44.5</td>\n",
       "      <td>0.646</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0.293</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>11</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>37</td>\n",
       "      <td>150</td>\n",
       "      <td>42.3</td>\n",
       "      <td>0.785</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>9</td>\n",
       "      <td>140</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>0.734</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>94</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>49.3</td>\n",
       "      <td>0.358</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>3</td>\n",
       "      <td>187</td>\n",
       "      <td>70</td>\n",
       "      <td>22</td>\n",
       "      <td>200</td>\n",
       "      <td>36.4</td>\n",
       "      <td>0.408</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>6</td>\n",
       "      <td>162</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.178</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>4</td>\n",
       "      <td>136</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.2</td>\n",
       "      <td>1.182</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>88</td>\n",
       "      <td>44</td>\n",
       "      <td>510</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.222</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>8</td>\n",
       "      <td>154</td>\n",
       "      <td>78</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>0.443</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>88</td>\n",
       "      <td>39</td>\n",
       "      <td>110</td>\n",
       "      <td>36.5</td>\n",
       "      <td>1.057</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.258</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>6</td>\n",
       "      <td>190</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>0.278</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>9</td>\n",
       "      <td>170</td>\n",
       "      <td>74</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.403</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>84</td>\n",
       "      <td>47</td>\n",
       "      <td>230</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>7</td>\n",
       "      <td>187</td>\n",
       "      <td>68</td>\n",
       "      <td>39</td>\n",
       "      <td>304</td>\n",
       "      <td>37.7</td>\n",
       "      <td>0.254</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>7</td>\n",
       "      <td>114</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.8</td>\n",
       "      <td>0.258</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "      <td>88</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.855</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>66</td>\n",
       "      <td>20</td>\n",
       "      <td>90</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.867</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>90</td>\n",
       "      <td>51</td>\n",
       "      <td>220</td>\n",
       "      <td>49.7</td>\n",
       "      <td>0.325</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.222</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              1       85             66             29        0  26.6   \n",
       "1              1       89             66             23       94  28.1   \n",
       "2              5      116             74              0        0  25.6   \n",
       "3             10      115              0              0        0  35.3   \n",
       "4              4      110             92              0        0  37.6   \n",
       "5             10      139             80              0        0  27.1   \n",
       "6              1      103             30             38       83  43.3   \n",
       "7              3      126             88             41      235  39.3   \n",
       "8              8       99             84              0        0  35.4   \n",
       "9              1       97             66             15      140  23.2   \n",
       "10            13      145             82             19      110  22.2   \n",
       "11             5      117             92              0        0  34.1   \n",
       "12             5      109             75             26        0  36.0   \n",
       "13             3       88             58             11       54  24.8   \n",
       "14             6       92             92              0        0  19.9   \n",
       "15            10      122             78             31        0  27.6   \n",
       "16             4      103             60             33      192  24.0   \n",
       "17            11      138             76              0        0  33.2   \n",
       "18             3      180             64             25       70  34.0   \n",
       "19             7      133             84              0        0  40.2   \n",
       "20             7      106             92             18        0  22.7   \n",
       "21             7      159             64              0        0  27.4   \n",
       "22             1      146             56              0        0  29.7   \n",
       "23             2       71             70             27        0  28.0   \n",
       "24             7      105              0              0        0   0.0   \n",
       "25             1      103             80             11       82  19.4   \n",
       "26             1      101             50             15       36  24.2   \n",
       "27             5       88             66             21       23  24.4   \n",
       "28             7      150             66             42      342  34.7   \n",
       "29             1       73             50             10        0  23.0   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "738            1      149             68             29      127  29.3   \n",
       "739            3      130             78             23       79  28.4   \n",
       "740            8      120             86              0        0  28.4   \n",
       "741            2      174             88             37      120  44.5   \n",
       "742            1      102             74              0        0  39.5   \n",
       "743           11      120             80             37      150  42.3   \n",
       "744            9      140             94              0        0  32.7   \n",
       "745            1      147             94             41        0  49.3   \n",
       "746            3      187             70             22      200  36.4   \n",
       "747            6      162             62              0        0  24.3   \n",
       "748            4      136             70              0        0  31.2   \n",
       "749            0      181             88             44      510  43.3   \n",
       "750            8      154             78             32        0  32.4   \n",
       "751            1      128             88             39      110  36.5   \n",
       "752            0      123             72              0        0  36.3   \n",
       "753            6      190             92              0        0  35.5   \n",
       "754            9      170             74             31        0  44.0   \n",
       "755            1      126             60              0        0  30.1   \n",
       "756            3       78             50             32       88  31.0   \n",
       "757            1      189             60             23      846  30.1   \n",
       "758            5      166             72             19      175  25.8   \n",
       "759            7      100              0              0        0  30.0   \n",
       "760            0      118             84             47      230  45.8   \n",
       "761            7      107             74              0        0  29.6   \n",
       "762            7      187             68             39      304  37.7   \n",
       "763            7      114             66              0        0  32.8   \n",
       "764            0      109             88             30        0  32.5   \n",
       "765            2      100             66             20       90  32.9   \n",
       "766            1      122             90             51      220  49.7   \n",
       "767            1      163             72              0        0  39.0   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  \n",
       "0                       0.351   31  \n",
       "1                       0.167   21  \n",
       "2                       0.201   30  \n",
       "3                       0.134   29  \n",
       "4                       0.191   30  \n",
       "5                       1.441   57  \n",
       "6                       0.183   33  \n",
       "7                       0.704   27  \n",
       "8                       0.388   50  \n",
       "9                       0.487   22  \n",
       "10                      0.245   57  \n",
       "11                      0.337   38  \n",
       "12                      0.546   60  \n",
       "13                      0.267   22  \n",
       "14                      0.188   28  \n",
       "15                      0.512   45  \n",
       "16                      0.966   33  \n",
       "17                      0.420   35  \n",
       "18                      0.271   26  \n",
       "19                      0.696   37  \n",
       "20                      0.235   48  \n",
       "21                      0.294   40  \n",
       "22                      0.564   29  \n",
       "23                      0.586   22  \n",
       "24                      0.305   24  \n",
       "25                      0.491   22  \n",
       "26                      0.526   26  \n",
       "27                      0.342   30  \n",
       "28                      0.718   42  \n",
       "29                      0.248   21  \n",
       "..                        ...  ...  \n",
       "738                     0.349   42  \n",
       "739                     0.323   34  \n",
       "740                     0.259   22  \n",
       "741                     0.646   24  \n",
       "742                     0.293   42  \n",
       "743                     0.785   48  \n",
       "744                     0.734   45  \n",
       "745                     0.358   27  \n",
       "746                     0.408   36  \n",
       "747                     0.178   50  \n",
       "748                     1.182   22  \n",
       "749                     0.222   26  \n",
       "750                     0.443   45  \n",
       "751                     1.057   37  \n",
       "752                     0.258   52  \n",
       "753                     0.278   66  \n",
       "754                     0.403   43  \n",
       "755                     0.349   47  \n",
       "756                     0.248   26  \n",
       "757                     0.398   59  \n",
       "758                     0.587   51  \n",
       "759                     0.484   32  \n",
       "760                     0.551   31  \n",
       "761                     0.254   31  \n",
       "762                     0.254   41  \n",
       "763                     0.258   42  \n",
       "764                     0.855   38  \n",
       "765                     0.867   28  \n",
       "766                     0.325   31  \n",
       "767                     1.222   33  \n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            1       85             66             29        0  26.6   \n",
       "1            1       89             66             23       94  28.1   \n",
       "2            5      116             74              0        0  25.6   \n",
       "3           10      115              0              0        0  35.3   \n",
       "4            4      110             92              0        0  37.6   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  \n",
       "0                     0.351   31  \n",
       "1                     0.167   21  \n",
       "2                     0.201   30  \n",
       "3                     0.134   29  \n",
       "4                     0.191   30  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = diabetes.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Outcome\n",
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.apply(pd.to_numeric)\n",
    "X_test = X_test.apply(pd.to_numeric)\n",
    "y_train = y_train.apply(pd.to_numeric)\n",
    "y_test = y_test.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 614 entries, 468 to 664\n",
      "Data columns (total 8 columns):\n",
      "Pregnancies                 614 non-null int64\n",
      "Glucose                     614 non-null int64\n",
      "BloodPressure               614 non-null int64\n",
      "SkinThickness               614 non-null int64\n",
      "Insulin                     614 non-null int64\n",
      "BMI                         614 non-null float64\n",
      "DiabetesPedigreeFunction    614 non-null float64\n",
      "Age                         614 non-null int64\n",
      "dtypes: float64(2), int64(6)\n",
      "memory usage: 43.2 KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt('train.csv', X_train,fmt='%i',delimiter=',')\n",
    "np.savetxt('train_o.csv', y_train,fmt='%i',delimiter=',')\n",
    "np.savetxt('test.csv', X_test,fmt='%i',delimiter=',')\n",
    "np.savetxt('test_o.csv', y_test,fmt='%i',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.genfromtxt('train.csv', dtype=int, delimiter=',')\n",
    "y_train = np.genfromtxt('train_o.csv', dtype=int, delimiter=',')\n",
    "X_test = np.genfromtxt('test.csv', dtype=int, delimiter=',')\n",
    "y_test = np.genfromtxt('test_o.csv', dtype=int, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "\n",
    "\n",
    "class NeuralNetMLP(object):\n",
    "    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels.\n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units.\n",
    "    l1 : float (default: 0.0)\n",
    "        Lambda value for L1-regularization.\n",
    "        No regularization if l1=0.0 (default)\n",
    "    l2 : float (default: 0.0)\n",
    "        Lambda value for L2-regularization.\n",
    "        No regularization if l2=0.0 (default)\n",
    "    epochs : int (default: 500)\n",
    "        Number of passes over the training set.\n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "    alpha : float (default: 0.0)\n",
    "        Momentum constant. Factor multiplied with the\n",
    "        gradient of the previous epoch t-1 to improve\n",
    "        learning speed\n",
    "        w(t) := w(t) - (grad(t) + alpha*grad(t-1))\n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        Normal gradient descent learning if k=1 (default).\n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=500, eta=0.001,\n",
    "                 alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=None):\n",
    "\n",
    "        np.random.seed(random_state)\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "\n",
    "    def _encode_labels(self, y, k):\n",
    "        \"\"\"Encode labels into one-hot representation\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : array, shape = [n_samples]\n",
    "            Target values.\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot : array, shape = (n_labels, n_samples)\n",
    "        \"\"\"\n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        w1 = np.random.uniform(-1.0, 1.0,\n",
    "                               size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = np.random.uniform(-1.0, 1.0,\n",
    "                               size=self.n_output*(self.n_hidden + 1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        return w1, w2\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\n",
    "        Uses scipy.special.expit to avoid overflow\n",
    "        error for very small input values z.\n",
    "        \"\"\"\n",
    "        # return 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "    def _sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the logistic function\"\"\"\n",
    "        sg = self._sigmoid(z)\n",
    "        return sg * (1.0 - sg)\n",
    "\n",
    "    def _add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        else:\n",
    "            raise AttributeError('`how` must be `column` or `row`')\n",
    "        return X_new\n",
    "\n",
    "    def _feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        z3 : array, shape = [n_output_units, n_samples]\n",
    "            Net input of output layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "        \"\"\"\n",
    "        a1 = self._add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='row')\n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) +\n",
    "                                np.sum(w2[:, 1:] ** 2))\n",
    "\n",
    "    def _L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() +\n",
    "                                np.abs(w2[:, 1:]).sum())\n",
    "\n",
    "    def _get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        output : array, shape = [n_output_units, n_samples]\n",
    "            Activation of the output layer (feedforward)\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float\n",
    "            Regularized cost.\n",
    "        \"\"\"\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        L1_term = self._L1_reg(self.l1, w1, w2)\n",
    "        L2_term = self._L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + L1_term + L2_term\n",
    "        return cost\n",
    "\n",
    "    def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, shape = [n_hidden_units, n_features]\n",
    "            Gradient of the weight matrix w1.\n",
    "        grad2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Gradient of the weight matrix w2.\n",
    "        \"\"\"\n",
    "        # backpropagation\n",
    "        sigma3 = a3 - y_enc\n",
    "        z2 = self._add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "\n",
    "        # regularize\n",
    "        grad1[:, 1:] += self.l2 * w1[:, 1:]\n",
    "        grad1[:, 1:] += self.l1 * np.sign(w1[:, 1:])\n",
    "        grad2[:, 1:] += self.l2 * w2[:, 1:]\n",
    "        grad2[:, 1:] += self.l1 * np.sign(w2[:, 1:])\n",
    "\n",
    "        return grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        if len(X.shape) != 2:\n",
    "            raise AttributeError('X must be a [n_samples, n_features] array.\\n'\n",
    "                                 'Use X[:,None] for 1-feature classification,'\n",
    "                                 '\\nor X[[i]] for 1-sample classification')\n",
    "\n",
    "        a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        y : array, shape = [n_samples]\n",
    "            Target class labels.\n",
    "        print_progress : bool (default: False)\n",
    "            Prints progress as the number of epochs\n",
    "            to stderr.\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.cost_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y, self.n_output)\n",
    "\n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self._feedforward(X_data[idx],\n",
    "                                                       self.w1,\n",
    "                                                       self.w2)\n",
    "                cost = self._get_cost(y_enc=y_enc[:, idx],\n",
    "                                      output=a3,\n",
    "                                      w1=self.w1,\n",
    "                                      w2=self.w2)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2,\n",
    "                                                  a3=a3, z2=z2,\n",
    "                                                  y_enc=y_enc[:, idx],\n",
    "                                                  w1=self.w1,\n",
    "                                                  w2=self.w2)\n",
    "\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> nn = NeuralNetMLP(n_output=10,\n",
    "                      n_features=X_train.shape[1],\n",
    "                      n_hidden=100,\n",
    "                      l2=0.1,\n",
    "                      l1=0.0,\n",
    "                      epochs=1000,\n",
    "                      eta=0.1,\n",
    "                      alpha=0.1,\n",
    "                      decrease_const=0.00001,\n",
    "                      shuffle=True,\n",
    "                      minibatches=50,\n",
    "                      random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2/1000C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:176: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:176: RuntimeWarning: invalid value encountered in multiply\n",
      "Epoch: 1000/1000"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NeuralNetMLP at 0x27507800b70>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> nn.fit(X_train, y_train, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x275087b42e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    ">>> import matplotlib.pyplot as plt\n",
    ">>> plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    ">>> plt.ylim([0, 2000])\n",
    ">>> plt.ylabel('Cost')\n",
    ">>> plt.xlabel('Epochs * 50')\n",
    ">>> plt.tight_layout()\n",
    ">>> plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzMrpf8P189C7Vvhy9gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x275087d5be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    ">>> batches = np.array_split(range(len(nn.cost_)), 1000)\n",
    ">>> cost_ary = np.array(nn.cost_)\n",
    ">>> cost_avgs = [np.mean(cost_ary[i]) for i in batches]\n",
    ">>> plt.plot(range(len(cost_avgs)),\n",
    "... cost_avgs,\n",
    "... color='red')\n",
    ">>> plt.ylim([0, 2000])\n",
    ">>> plt.ylabel('Cost')\n",
    ">>> plt.xlabel('Epochs')\n",
    ">>> plt.tight_layout()\n",
    ">>> plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 71.66%\n"
     ]
    }
   ],
   "source": [
    ">>> y_train_pred = nn.predict(X_train)\n",
    ">>> acc = np.sum(y_train == y_train_pred, axis=0) / X_train.shape[0]\n",
    ">>> print('Training accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 74.68%\n"
     ]
    }
   ],
   "source": [
    ">>> y_test_pred = nn.predict(X_test)\n",
    ">>> acc = np.sum(y_test == y_test_pred, axis=0) / X_test.shape[0]\n",
    ">>> print('Testing accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
